{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# sys.path.append('devel/lib/python3/dist-packages/theconstruct_msgs/msg/')\n",
                "# sys.path.append('/home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/moveo_training')\n",
                "# sys.path.append('/home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib/python3/dist-packages/openai_ros/msg')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import base64\n",
                "import imageio\n",
                "import IPython\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import reverb\n",
                "import tempfile\n",
                "import PIL.Image\n",
                "\n",
                "import tensorflow as tf\n",
                "from tf_agents.environments import suite_gym\n",
                "from tf_agents.agents.ddpg import critic_network\n",
                "from tf_agents.agents.sac import sac_agent\n",
                "from tf_agents.agents.sac import tanh_normal_projection_network\n",
                "from tf_agents.environments import suite_pybullet\n",
                "from tf_agents.metrics import py_metrics\n",
                "from tf_agents.networks import actor_distribution_network\n",
                "from tf_agents.policies import greedy_policy\n",
                "from tf_agents.policies import py_tf_eager_policy\n",
                "from tf_agents.policies import random_py_policy\n",
                "from tf_agents.policies import tf_py_policy\n",
                "from tf_agents.replay_buffers import reverb_replay_buffer\n",
                "from tf_agents.replay_buffers import reverb_utils\n",
                "from tf_agents.train import actor\n",
                "from tf_agents.train import learner\n",
                "from tf_agents.train import triggers\n",
                "from tf_agents.train.utils import spec_utils\n",
                "from tf_agents.train.utils import strategy_utils\n",
                "from tf_agents.train.utils import train_utils\n",
                "import rospy\n",
                "\n",
                "from src.moveo_training.src.moveo_training import moveo_inverse_kinematic\n",
                "tempdir = tempfile.gettempdir()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-31 18:26:09.314503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-31 18:26:09.314535: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Failed to import pyassimp, see https://github.com/ros-planning/moveit/issues/86 for more info\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "num_iterations =10000# @param {type:\"integer\"}\n",
                "\n",
                "initial_collect_steps = 200 # @param {type:\"integer\"}\n",
                "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
                "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
                "\n",
                "batch_size = 256 # @param {type:\"integer\"}\n",
                "\n",
                "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "target_update_tau = 0.005 # @param {type:\"number\"}\n",
                "target_update_period = 1 # @param {type:\"number\"}\n",
                "gamma = 0.99 # @param {type:\"number\"}\n",
                "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
                "\n",
                "actor_fc_layer_params = (256, 256)\n",
                "critic_joint_fc_layer_params = (256, 256)\n",
                "\n",
                "log_interval = 100 # @param {type:\"integer\"}\n",
                "\n",
                "num_eval_episodes = 1 # @param {type:\"integer\"}\n",
                "eval_interval = 100 # @param {type:\"integer\"}\n",
                "\n",
                "policy_save_interval = 500 # @param {type:\"integer\"}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "rospy.init_node(\"train_moveo\")\n",
                "env_name = 'MoveoIK-v0' # @param {type:\"string\"}\n",
                "env = suite_gym.load('MoveoIK-v0')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "env.reset()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print('Observation Spec:')\n",
                "print(env.time_step_spec().observation)\n",
                "print('Action Spec:')\n",
                "print(env.action_spec())\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "collect_env = suite_gym.load(env_name)\n",
                "eval_env = suite_gym.load(env_name)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "use_gpu = False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "observation_spec, action_spec, time_step_spec = (\n",
                "      spec_utils.get_tensor_specs(collect_env))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "with strategy.scope():\n",
                "  critic_net = critic_network.CriticNetwork(\n",
                "        (observation_spec, action_spec),\n",
                "        observation_fc_layer_params=None,\n",
                "        action_fc_layer_params=None,\n",
                "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
                "        kernel_initializer='glorot_uniform',\n",
                "        last_kernel_initializer='glorot_uniform')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "with strategy.scope():\n",
                "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
                "      observation_spec,\n",
                "      action_spec,\n",
                "      fc_layer_params=actor_fc_layer_params,\n",
                "      continuous_projection_net=(\n",
                "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "with strategy.scope():\n",
                "  train_step = train_utils.create_train_step()\n",
                "\n",
                "  tf_agent = sac_agent.SacAgent(\n",
                "        time_step_spec,\n",
                "        action_spec,\n",
                "        actor_network=actor_net,\n",
                "        critic_network=critic_net,\n",
                "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=actor_learning_rate),\n",
                "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=critic_learning_rate),\n",
                "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=alpha_learning_rate),\n",
                "        target_update_tau=target_update_tau,\n",
                "        target_update_period=target_update_period,\n",
                "        td_errors_loss_fn=tf.math.squared_difference,\n",
                "        gamma=gamma,\n",
                "        reward_scale_factor=reward_scale_factor,\n",
                "        train_step_counter=train_step)\n",
                "\n",
                "  tf_agent.initialize()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "table_name = 'uniform_table'\n",
                "table = reverb.Table(\n",
                "    table_name,\n",
                "    max_size=replay_buffer_capacity,\n",
                "    sampler=reverb.selectors.Uniform(),\n",
                "    remover=reverb.selectors.Fifo(),\n",
                "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
                "\n",
                "reverb_server = reverb.Server([table])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
                "    tf_agent.collect_data_spec,\n",
                "    sequence_length=2,\n",
                "    table_name=table_name,\n",
                "    local_server=reverb_server)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataset = reverb_replay.as_dataset(\n",
                "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
                "experience_dataset_fn = lambda: dataset"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "tf_eval_policy = tf_agent.policy\n",
                "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
                "  tf_eval_policy, use_tf_function=True)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "tf_collect_policy = tf_agent.collect_policy\n",
                "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
                "  tf_collect_policy, use_tf_function=True)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "random_policy = random_py_policy.RandomPyPolicy(\n",
                "  collect_env.time_step_spec(), collect_env.action_spec())\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
                "  reverb_replay.py_client,\n",
                "  table_name,\n",
                "  sequence_length=2,\n",
                "  stride_length=1)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "initial_collect_actor = actor.Actor(\n",
                "  collect_env,\n",
                "  random_policy,\n",
                "  train_step,\n",
                "  steps_per_run=initial_collect_steps,\n",
                "  observers=[rb_observer])\n",
                "initial_collect_actor.run()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "env_step_metric = py_metrics.EnvironmentSteps()\n",
                "collect_actor = actor.Actor(\n",
                "  collect_env,\n",
                "  collect_policy,\n",
                "  train_step,\n",
                "  steps_per_run=1,\n",
                "  metrics=actor.collect_metrics(10),\n",
                "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
                "  observers=[rb_observer, env_step_metric])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "eval_actor = actor.Actor(\n",
                "  eval_env,\n",
                "  eval_policy,\n",
                "  train_step,\n",
                "  episodes_per_run=num_eval_episodes,\n",
                "  metrics=actor.eval_metrics(num_eval_episodes),\n",
                "  summary_dir=os.path.join(tempdir, 'eval'),\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "\n",
                "# Triggers to save the agent's policy checkpoints.\n",
                "learning_triggers = [\n",
                "    triggers.PolicySavedModelTrigger(\n",
                "        saved_model_dir,\n",
                "        tf_agent,\n",
                "        train_step,\n",
                "        interval=policy_save_interval),\n",
                "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
                "]\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "agent_learner = learner.Learner(\n",
                "  tempdir,\n",
                "  train_step,\n",
                "  tf_agent,\n",
                "  experience_dataset_fn,\n",
                "  triggers=learning_triggers)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "def get_eval_metrics():\n",
                "  print(\"Starte evaluierung\")\n",
                "  eval_actor.run()\n",
                "  results = {}\n",
                "  for metric in eval_actor.metrics:\n",
                "    results[metric.name] = metric.result()\n",
                "  print(\"Evaluierung beendete\")\n",
                "  return results\n",
                "\n",
                "metrics = get_eval_metrics()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "def log_eval_metrics(step, metrics):\n",
                "  eval_results = (', ').join(\n",
                "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
                "  print('step = {0}: {1}'.format(step, eval_results))\n",
                "\n",
                "log_eval_metrics(0, metrics)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Reset the train step\n",
                "tf_agent.train_step_counter.assign(0)\n",
                "# Evaluate the agent's policy once before training.\n",
                "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
                "returns = [avg_return]\n",
                "avg_episodes_length = get_eval_metrics()[\"AverageEpisodeLength\"]\n",
                "episodes_lengths = [avg_episodes_length]\n",
                "for _ in range(num_iterations):\n",
                "  # Training.\n",
                "  collect_actor.run()\n",
                "  loss_info = agent_learner.run(iterations=1)\n",
                "  # Evaluating.\n",
                "  step = agent_learner.train_step_numpy\n",
                "  if eval_interval and step % eval_interval == 0:\n",
                "    metrics = get_eval_metrics()\n",
                "    log_eval_metrics(step, metrics)\n",
                "   \n",
                "\n",
                "    returns.append(metrics[\"AverageReturn\"])\n",
                "    episodes_lengths.append(metrics[\"AverageEpisodeLength\"])\n",
                "  if log_interval and step % log_interval == 0:\n",
                "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "rb_observer.close()\n",
                "reverb_server.stop()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "returns"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "steps = range(0, num_iterations + 1, eval_interval)\n",
                "plt.plot(steps, returns)\n",
                "plt.ylabel('Average Return')\n",
                "plt.xlabel('Step')\n",
                "plt.ylim()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# from IPython.display import set_matplotlib_formats\n",
                "# %matplotlib inline\n",
                "# set_matplotlib_formats('svg')\n",
                "\n",
                "\n",
                "# fig,ax = plt.subplots()\n",
                "# plt.plot(steps,returns )\n",
                "# plt.ylabel('Reward')\n",
                "# plt.xlabel('Episode')\n",
                "# plt.ylim()\n",
                "# image_format = 'svg' # e.g .png, .svg, etc.\n",
                "# image_name = 'avgReward_6800E_100S_RewardExplotation.svg'\n",
                "\n",
                "# fig.savefig(image_name, format=image_format, dpi=1200)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "interpreter": {
            "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}