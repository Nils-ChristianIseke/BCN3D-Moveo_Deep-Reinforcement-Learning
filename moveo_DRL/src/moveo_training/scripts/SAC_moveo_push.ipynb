{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import sys\n",
                "sys.executable\n",
                "import os\n",
                "os.getcwd()\n",
                "sys.path.append('devel/lib/python3/dist-packages/theconstruct_msgs/msg/')\n",
                "sys.path.append('/home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/src')\n",
                "sys.path.append('/home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/src/moveo_training/scripts')\n",
                "sys.path.append('/home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib/python3/dist-packages/openai_ros/msg')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import base64\n",
                "import imageio\n",
                "import IPython\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import reverb\n",
                "import tempfile\n",
                "import PIL.Image\n",
                "\n",
                "import tensorflow as tf\n",
                "import moveo_push\n",
                "from tf_agents.environments import suite_gym\n",
                "from tf_agents.agents.ddpg import critic_network\n",
                "from tf_agents.agents.sac import sac_agent\n",
                "from tf_agents.agents.sac import tanh_normal_projection_network\n",
                "from tf_agents.environments import suite_pybullet\n",
                "from tf_agents.metrics import py_metrics\n",
                "from tf_agents.networks import actor_distribution_network\n",
                "from tf_agents.policies import greedy_policy\n",
                "from tf_agents.policies import py_tf_eager_policy\n",
                "from tf_agents.policies import random_py_policy\n",
                "from tf_agents.replay_buffers import reverb_replay_buffer\n",
                "from tf_agents.replay_buffers import reverb_utils\n",
                "from tf_agents.train import actor\n",
                "from tf_agents.train import learner\n",
                "from tf_agents.train import triggers\n",
                "from tf_agents.train.utils import spec_utils\n",
                "from tf_agents.train.utils import strategy_utils\n",
                "from tf_agents.train.utils import train_utils\n",
                "import rospy\n",
                "from gym.envs.registration import register\n",
                "import gym\n",
                "tempdir = tempfile.gettempdir()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-21 13:01:05.996503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:05.996544: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Failed to import pyassimp, see https://github.com/ros-planning/moveit/issues/86 for more info\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "tempdir"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'/tmp'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "num_iterations = 5000 # @param {type:\"integer\"}\n",
                "\n",
                "initial_collect_steps = 500 # @param {type:\"integer\"}\n",
                "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
                "replay_buffer_capacity = 1000 # @param {type:\"integer\"}\n",
                "\n",
                "batch_size = 256 # @param {type:\"integer\"}\n",
                "\n",
                "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
                "target_update_tau = 0.005 # @param {type:\"number\"}\n",
                "target_update_period = 1 # @param {type:\"number\"}\n",
                "gamma = 0.99 # @param {type:\"number\"}\n",
                "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
                "\n",
                "actor_fc_layer_params = (256, 256)\n",
                "critic_joint_fc_layer_params = (256, 256)\n",
                "\n",
                "log_interval = 50# @param {type:\"integer\"}\n",
                "\n",
                "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
                "eval_interval = 100 # @param {type:\"integer\"}\n",
                "\n",
                "policy_save_interval = 1000 # @param {type:\"integer\"}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# env_name = 'MoveoIK-v0'\n",
                "# if env_name in gym.envs.registry.env_specs:\n",
                "#     del gym.envs.registry.env_specs[env_name]\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# env_name = 'MoveoIK-v0'\n",
                "# if env_name in gym.envs.registry.env_specs:\n",
                "#     del gym.envs.registry.env_specs[env_name]\n",
                "\n",
                "# register(\n",
                "#         id=env_name ,\n",
                "#         entry_point='moveo_inverse_kinematic_2:MoveoIKEnv',\n",
                "#         max_episode_steps=max_episode_steps,\n",
                "#     )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "rospy.init_node(\"train_moveo_push\")\n",
                "env_name = 'MoveoPush-v0' # @param {type:\"string\"}\n",
                "env = suite_gym.load(env_name)\n",
                "moveo_push"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "###### Setting Group Moveit with 30 seconds wait...\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[ERROR] [1629543670.091612, 587462.461000]: NOT Initialising Simulation Physics Parameters\n",
                        "[WARN] [1629543670.105613, 587464.301000]: Start Init ControllersConnection\n",
                        "[WARN] [1629543670.107555, 587464.381000]: END Init ControllersConnection\n",
                        "/home/nils/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
                        "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "env.reset()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "TimeStep(\n",
                            "{'discount': array(1., dtype=float32),\n",
                            " 'observation': array([0.3, 0.3, 0.3], dtype=float32),\n",
                            " 'reward': array(0., dtype=float32),\n",
                            " 'step_type': array(0, dtype=int32)})"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "print('Observation Spec:')\n",
                "print(env.time_step_spec().observation)\n",
                "print('Action Spec:')\n",
                "print(env.action_spec())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Observation Spec:\n",
                        "BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=-1.0, maximum=1.0)\n",
                        "Action Spec:\n",
                        "BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='action', minimum=-3.140000104904175, maximum=3.140000104904175)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "collect_env = suite_gym.load(env_name)\n",
                "eval_env = suite_gym.load(env_name)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "###### Setting Group Moveit with 30 seconds wait...\n",
                        "###### Setting Group Moveit with 30 seconds wait...\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[ERROR] [1629543671.297824, 587541.871000]: NOT Initialising Simulation Physics Parameters\n",
                        "[WARN] [1629543671.316260, 587544.181000]: Start Init ControllersConnection\n",
                        "[WARN] [1629543671.317721, 587544.191000]: END Init ControllersConnection\n",
                        "[ERROR] [1629543671.431914, 587553.331000]: NOT Initialising Simulation Physics Parameters\n",
                        "[WARN] [1629543671.451190, 587554.401000]: Start Init ControllersConnection\n",
                        "[WARN] [1629543671.452949, 587554.401000]: END Init ControllersConnection\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "use_gpu = True\n",
                "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
                        "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
                        "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-21 13:01:11.534775: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
                        "2021-08-21 13:01:11.565962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
                        "2021-08-21 13:01:11.566533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
                        "pciBusID: 0000:01:00.0 name: GeForce GTX 970 computeCapability: 5.2\n",
                        "coreClock: 1.2155GHz coreCount: 13 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 208.91GiB/s\n",
                        "2021-08-21 13:01:11.566837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.567601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.567825: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.567917: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.568005: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.568090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.568173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.568256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nils/Documents/Studium/Bachelorarbeit/praktischer_Teil/moveo_DRL/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
                        "2021-08-21 13:01:11.568266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
                        "Skipping registering GPU devices...\n",
                        "2021-08-21 13:01:11.570344: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2021-08-21 13:01:11.570720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
                        "2021-08-21 13:01:11.570731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "observation_spec, action_spec, time_step_spec = (\n",
                "      spec_utils.get_tensor_specs(collect_env))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "with strategy.scope():\n",
                "  critic_net = critic_network.CriticNetwork(\n",
                "        (observation_spec, action_spec),\n",
                "        observation_fc_layer_params=None,\n",
                "        action_fc_layer_params=None,\n",
                "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
                "        kernel_initializer='glorot_uniform',\n",
                "        last_kernel_initializer='glorot_uniform')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "with strategy.scope():\n",
                "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
                "      observation_spec,\n",
                "      action_spec,\n",
                "      fc_layer_params=actor_fc_layer_params,\n",
                "      continuous_projection_net=(\n",
                "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "with strategy.scope():\n",
                "  train_step = train_utils.create_train_step()\n",
                "\n",
                "  tf_agent = sac_agent.SacAgent(\n",
                "        time_step_spec,\n",
                "        action_spec,\n",
                "        actor_network=actor_net,\n",
                "        critic_network=critic_net,\n",
                "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=actor_learning_rate),\n",
                "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=critic_learning_rate),\n",
                "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
                "            learning_rate=alpha_learning_rate),\n",
                "        target_update_tau=target_update_tau,\n",
                "        target_update_period=target_update_period,\n",
                "        td_errors_loss_fn=tf.math.squared_difference,\n",
                "        gamma=gamma,\n",
                "        reward_scale_factor=reward_scale_factor,\n",
                "        train_step_counter=train_step)\n",
                "\n",
                "  tf_agent.initialize()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "table_name = 'uniform_table'\n",
                "table = reverb.Table(\n",
                "    table_name,\n",
                "    max_size=replay_buffer_capacity,\n",
                "    sampler=reverb.selectors.Uniform(),\n",
                "    remover=reverb.selectors.Fifo(),\n",
                "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
                "\n",
                "reverb_server = reverb.Server([table])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmpk1ez9lgp.\n",
                        "[reverb/cc/platform/tfrecord_checkpointer.cc:378] Loading latest checkpoint from /tmp/tmpk1ez9lgp\n",
                        "[reverb/cc/platform/default/server.cc:54] Started replay server on port 21388\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
                "    tf_agent.collect_data_spec,\n",
                "    sequence_length=2,\n",
                "    table_name=table_name,\n",
                "    local_server=reverb_server)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "dataset = reverb_replay.as_dataset(\n",
                "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
                "experience_dataset_fn = lambda: dataset"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "tf_eval_policy = tf_agent.policy\n",
                "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
                "  tf_eval_policy, use_tf_function=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "tf_collect_policy = tf_agent.collect_policy\n",
                "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
                "  tf_collect_policy, use_tf_function=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "random_policy = random_py_policy.RandomPyPolicy(\n",
                "  collect_env.time_step_spec(), collect_env.action_spec())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
                "  reverb_replay.py_client,\n",
                "  table_name,\n",
                "  sequence_length=2,\n",
                "  stride_length=1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "initial_collect_actor = actor.Actor(\n",
                "  collect_env,\n",
                "  random_policy,\n",
                "  train_step,\n",
                "  steps_per_run=initial_collect_steps,\n",
                "  observers=[rb_observer])\n",
                "initial_collect_actor.run()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\u001b[31m[ERROR] [1629543675.384781539, 587889.021000000]: Got a callback on a goalHandle that we're not tracking.                 This is an internal SimpleActionClient/ActionClient bug.                 This could also be a GoalID collision\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543686.629164148, 588779.811000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543687.320368011, 588837.911000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543690.170849259, 589093.211000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543696.639217567, 589632.731000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543707.961566874, 590678.031000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n",
                        "\u001b[33m[ WARN] [1629543708.524455245, 590730.991000000]: Fail: ABORTED: No motion plan found. No execution attempted.\u001b[0m\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "env_step_metric = py_metrics.EnvironmentSteps()\n",
                "collect_actor = actor.Actor(\n",
                "  collect_env,\n",
                "  collect_policy,\n",
                "  train_step,\n",
                "  steps_per_run=1,\n",
                "  metrics=actor.collect_metrics(10),\n",
                "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
                "  observers=[rb_observer, env_step_metric])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "eval_actor = actor.Actor(\n",
                "  eval_env,\n",
                "  eval_policy,\n",
                "  train_step,\n",
                "  episodes_per_run=num_eval_episodes,\n",
                "  metrics=actor.eval_metrics(num_eval_episodes),\n",
                "  summary_dir=os.path.join(tempdir, 'eval'),\n",
                ")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
                "\n",
                "# Triggers to save the agent's policy checkpoints.\n",
                "learning_triggers = [\n",
                "    triggers.PolicySavedModelTrigger(\n",
                "        saved_model_dir,\n",
                "        tf_agent,\n",
                "        train_step,\n",
                "        interval=policy_save_interval),\n",
                "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
                "]\n",
                "agent_learner = learner.Learner(\n",
                "  tempdir,\n",
                "  train_step,\n",
                "  tf_agent,\n",
                "  experience_dataset_fn,\n",
                "  triggers=learning_triggers)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "def get_eval_metrics():\n",
                "  print(\"Starte evaluierung\")\n",
                "  eval_actor.run()\n",
                "  results = {}\n",
                "  for metric in eval_actor.metrics:\n",
                "    results[metric.name] = metric.result()\n",
                "  print(\"Evaluierung beendete\")\n",
                "  return results\n",
                "\n",
                "metrics = get_eval_metrics()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n",
                "def log_eval_metrics(step, metrics):\n",
                "  eval_results = (', ').join(\n",
                "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
                "  print('step = {0}: {1}'.format(step, eval_results))\n",
                "\n",
                "log_eval_metrics(0, metrics)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "try:\n",
                "  %%time\n",
                "except:\n",
                "  pass\n",
                "\n",
                "\n",
                "# Reset the train step\n",
                "tf_agent.train_step_counter.assign(0)\n",
                "\n",
                "# Evaluate the agent's policy once before training.\n",
                "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
                "returns = [avg_return]\n",
                "i =0\n",
                "for _ in range(num_iterations):\n",
                "  # Training.\n",
                "  i +=1\n",
                "  collect_actor.run()\n",
                "  loss_info = agent_learner.run(iterations=1)\n",
                "  \n",
                "  # Evaluating.\n",
                "  step = agent_learner.train_step_numpy\n",
                "  if eval_interval and step % eval_interval == 0:\n",
                "    metrics = get_eval_metrics()\n",
                "    log_eval_metrics(step, metrics)\n",
                "    returns.append(metrics[\"AverageReturn\"])\n",
                "\n",
                "  if log_interval and step % log_interval == 0:\n",
                "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
                "rb_observer.close()\n",
                "reverb_server.stop()\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "steps = range(0, num_iterations + 1, eval_interval)\n",
                "plt.plot(steps, returns)\n",
                "plt.ylabel('Average Return')\n",
                "plt.xlabel('Step')\n",
                "plt.ylim()\n",
                "returns"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "get_eval_metrics()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "shutdown request: [/train_moveo_her] Reason: new node registered with same name\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plt.scatter(steps,returns)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "interpreter": {
            "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}